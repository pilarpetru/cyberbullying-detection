{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Obtención de los datos"
      ],
      "metadata": {
        "id": "3nSLbcOX07Ym"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_eGsWExU06NM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import json\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "# para la tabla\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Bearer Token\n",
        "bearer_token = $bearer_token$"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# función autenticación\n",
        "def bearer_oauth(r):\n",
        "  r.headers['Authorization'] = f\"Bearer {bearer_token}\"\n",
        "  r.headers['User-Agent'] = \"pilar\"\n",
        "  return r"
      ],
      "metadata": {
        "id": "2tD-VfAL1Ew5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# función obtención tweets\n",
        "def get_tweets():\n",
        "  url = \"https://api.twitter.com/2/tweets/search/recent\"\n",
        "  response = requests.request(\"GET\", url, auth=bearer_oauth, params={'query': 'has:images context:46.781974597491040257 OR context:131.847900493514891265 lang:en -is:retweet -is:reply -is:quote ', \n",
        "                                                                     'max_results': 100, \n",
        "                                                                     'expansions': 'attachments.media_keys', \n",
        "                                                                     'tweet.fields': 'author_id,conversation_id,entities,referenced_tweets,public_metrics,created_at',\n",
        "                                                                     'media.fields': 'url,alt_text,public_metrics,preview_image_url'})\n",
        "  info = json.loads(response.text)\n",
        "\n",
        "  infoo.append(info)\n",
        "  data.append(info['data'])\n",
        "  media.append(info['includes']['media'])"
      ],
      "metadata": {
        "id": "L-ZqbHHA1P3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# obtención de los 1000 tweets\n",
        "data = []\n",
        "media = []\n",
        "infoo = []\n",
        "for i in range(10):\n",
        "  get_tweets()"
      ],
      "metadata": {
        "id": "dwvZ6H-i1hdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Almacenamiento de los datos"
      ],
      "metadata": {
        "id": "WzkdH9zO1jyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "table_1 = [[], [], [], [], [], [], [], []]\n",
        "for i in range(len(data)):\n",
        "  for tweet in data[i]:\n",
        "    table_1[0].append(tweet['id'])\n",
        "    table_1[1].append(tweet['text'])\n",
        "    table_1[2].append(tweet['author_id'])\n",
        "    try:\n",
        "      hashtags = []\n",
        "      for h in tweet['entities']['hashtags']:\n",
        "        hashtags.append(h['tag'])\n",
        "      table_1[3].append(hashtags)\n",
        "    except:\n",
        "      table_1[3].append(\"-\")\n",
        "\n",
        "    try:\n",
        "      mentions = []\n",
        "      for m in tweet['entities']['mentions']:\n",
        "        mentions.append(m['id'])\n",
        "      table_1[4].append(mentions)\n",
        "    except:\n",
        "      table_1[4].append(\"-\")\n",
        "\n",
        "    try:\n",
        "      table_1[5].append(tweet['referenced_tweets'][0]['type'])\n",
        "      table_1[6].append(tweet['referenced_tweets'][0]['id'])\n",
        "    except:\n",
        "      table_1[5].append('original')\n",
        "      table_1[6].append('-')\n",
        "\n",
        "    try:\n",
        "      table_1[7].append(tweet['entities']['urls'][0]['images'][0]['url'])\n",
        "    except:\n",
        "      try:\n",
        "        table_1[7].append(tweet['entities']['urls']['url'])\n",
        "      except:\n",
        "        table_1[7].append('-')\n",
        "\n",
        "# datos se usa para el clasificador\n",
        "datos = {'id':table_1[0], 'texto':table_1[1], 'usuario':table_1[2], 'hashtag':table_1[3], 'menciones':table_1[4], 'tipo':table_1[5], 'id_tw_original':table_1[6], 'img_url':table_1[7]}\n",
        "\n",
        "# estructura_final = {'id':table_2[0], 'conversation_id':table_2[1], 'hashtags':table_2[2], 'mentions':table_2[3], 'RT':table_2[4], 'id_tw_original':table_2[5], 'likes':table_2[6], 'retweets':table_2[7], 'replies':table_2[8], 'quotes':table_2[9]}"
      ],
      "metadata": {
        "id": "kSrY0Qfj1kZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(datos,columns = ['id', 'texto', 'usuario', 'hashtag', 'menciones', 'tipo', 'id_tw_original', 'img_url'])\n",
        "df"
      ],
      "metadata": {
        "id": "jZNWxcV81pLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess"
      ],
      "metadata": {
        "id": "cHVg_6rb1q28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install demoji\n",
        "import demoji\n",
        "demoji.download_codes()\n",
        "!pip install contractions\n",
        "import contractions"
      ],
      "metadata": {
        "id": "e8DFf2Kc1snM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# contracciones\n",
        "df['cleaned'] = df['texto'].apply(lambda x: contractions.fix(x, slang=True))\n",
        "\n",
        "# emoticonos\n",
        "df['cleaned'] = df['cleaned'].apply(lambda x: demoji.replace_with_desc(x))\n",
        "df"
      ],
      "metadata": {
        "id": "bBHjawgB1uMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Tweet: ')\n",
        "df.texto[9]"
      ],
      "metadata": {
        "id": "WE48xqo81wf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Después de eliminar contracciones y describir emoticonos:')\n",
        "df.cleaned[9]"
      ],
      "metadata": {
        "id": "JY6RbZep1xrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# se elimina retweets y menciones\n",
        "df['cleaned'] = df['cleaned'].str.replace(\"RT @[\\w]*\", \" \", regex=True)\n",
        "df['cleaned'] = df['cleaned'].str.replace(\"@[\\w]*\", \" \", regex=True)\n",
        "\n",
        "# se eliminan urls\n",
        "df['cleaned'] = df['cleaned'].str.replace(\"\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*\", \" \", regex=True)\n",
        "\n",
        "df['cleaned'] = df['cleaned'].str.replace(\"[^a-zA-Z]\", \" \", regex=True)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "HJzZSs5l1zKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade google-cloud-vision\n",
        "from google.cloud import vision\n",
        "from google.oauth2 import service_account\n",
        "from google.colab import files\n",
        "import json\n",
        "import pandas as pd\n",
        "import io"
      ],
      "metadata": {
        "id": "s1ofDYZN10c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# upload el json con el token generado en google cloud\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "BTmYj_8R11_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for tweet in df['img_url']:\n",
        "  print(tweet)"
      ],
      "metadata": {
        "id": "kluKKn5t17LB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "credentials = service_account.Credentials.from_service_account_file('careful-drummer-353422-aacd09dd194c.json')"
      ],
      "metadata": {
        "id": "G8NGJyot18k0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = vision.ImageAnnotatorClient(credentials=credentials)\n",
        "for i in range(1000):\n",
        "  tweet = df['img_url'][i]\n",
        "  if tweet != '-':\n",
        "    response = client.annotate_image({'image': {'source': {'image_uri': tweet}}, 'features': [{'type_': vision.Feature.Type.LABEL_DETECTION}]})\n",
        "    labels = response.label_annotations\n",
        "\n",
        "    description = []\n",
        "    for label in labels:\n",
        "      description.append(label.description)\n",
        "\n",
        "    text = ' '.join(d for d in description)\n",
        "\n",
        "    df['descripcion img'][i] = text\n",
        "  else:\n",
        "    df['descripcion img'][i] = '-'\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "vKEeurwM1-CR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['descripcion img'][0]"
      ],
      "metadata": {
        "id": "CDtfdEd_1_hJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['cleaned'] = df['cleaned'] + df['descripcion img']\n",
        "df.head()"
      ],
      "metadata": {
        "id": "-noPlDd_2AmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['cleaned'][0]"
      ],
      "metadata": {
        "id": "yTd0zQCa2BzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop('img_url', axis=1, inplace=True)\n",
        "df.drop('descripcion img', axis=1, inplace=True)\n",
        "df"
      ],
      "metadata": {
        "id": "f0m7cDRN2CyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization"
      ],
      "metadata": {
        "id": "geNL_rVL2EMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk import word_tokenize, pos_tag, pos_tag_sents\n",
        "from collections import defaultdict\n",
        "tag_map = defaultdict(lambda : wn.NOUN)\n",
        "tag_map['J'] = wn.ADJ\n",
        "tag_map['V'] = wn.VERB\n",
        "tag_map['R'] = wn.ADV\n",
        "\n",
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "sentences = df['cleaned'].apply(lambda x: pos_tag(word_tokenize(x)))\n",
        "\n",
        "for s in range(0, len(sentences)):\n",
        "  words = []\n",
        "  for w in range(0, len(sentences[s])):\n",
        "    token = sentences[s][w][0]\n",
        "    tag = sentences[s][w][1]\n",
        "    words.append(wnl.lemmatize(token, tag_map[tag[0]]))\n",
        "  sentences[s] = \" \".join(words)\n",
        "\n",
        "df['lemmatization'] = sentences\n",
        "df.head()"
      ],
      "metadata": {
        "id": "RAwAXUeN2FlI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Doc2vec and Logistic Regression"
      ],
      "metadata": {
        "id": "q4SMoK1x2Hs7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import joblib\n",
        "from tqdm import tqdm\n",
        "from gensim.models import Doc2Vec\n",
        "from gensim.models import doc2vec"
      ],
      "metadata": {
        "id": "YieR3pXv2Kot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importar el modelo pre entrenado\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "9beGlNmH2K8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = joblib.load(\"model (2).pkl\")"
      ],
      "metadata": {
        "id": "mh32VHy42PI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def label_sentences(corpus, label_type):\n",
        "    labeled = []\n",
        "    for i, v in enumerate(corpus):\n",
        "        label = label_type + '_' + str(i)\n",
        "        labeled.append(doc2vec.TaggedDocument(v.split(), [label]))\n",
        "    return labeled"
      ],
      "metadata": {
        "id": "EK5Ujt862QQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_data = label_sentences(df.lemmatization, 'Test')"
      ],
      "metadata": {
        "id": "fR2FLyaV2RRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, min_count=1, alpha=0.065, min_alpha=0.065)\n",
        "model_dbow.build_vocab([x for x in tqdm(all_data)])"
      ],
      "metadata": {
        "id": "9kb2xh7P2ShO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vectors(model, corpus_size, vectors_size, vectors_type):\n",
        "    vectors = np.zeros((corpus_size, vectors_size))\n",
        "    for i in range(0, corpus_size):\n",
        "        prefix = vectors_type + '_' + str(i)\n",
        "        vectors[i] = model.docvecs[prefix]\n",
        "    return vectors"
      ],
      "metadata": {
        "id": "FpWz_sqY2Tfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectors_dbow = get_vectors(model_dbow, len(df['texto']), 300, 'Test')"
      ],
      "metadata": {
        "id": "2a0yLxZu2Unt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1000):\n",
        "  if classifier.predict(vectors_dbow)[i].startswith(\"POS\"):\n",
        "    df['label'][i] = 0\n",
        "  else:\n",
        "    df['label'][i] = 1\n",
        "#df['label'] = classifier.predict(vectors_dbow)"
      ],
      "metadata": {
        "id": "QmGlYvQj2ViU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "dlDn4ONz2Whh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing positive tweets"
      ],
      "metadata": {
        "id": "wUGy-4G42XZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t = [[], [], [], [], [], [], [], [], [], [], []]\n",
        "for i in range(1000):\n",
        "  if df['label'][i] == 1:\n",
        "    t[0].append(df['id'][i])\n",
        "    t[1].append(df['usuario'][i])\n",
        "    t[2].append(df['tipo'][i])\n",
        "    t[3].append(df['id_tw_original'][i])\n",
        "    t[4].append(df['label'][i])\n",
        "    t[5].append(df['menciones'][i])\n",
        "\n",
        "d = {'id':t[0], 'usuario':t[1], 'tipo':t[2], 'id_tw_original':t[3], 'label':t[4], 'menciones':t[5]}\n",
        "df2 = pd.DataFrame(d, columns=['id', 'usuario', 'tipo', 'id_tw_original', 'label', 'menciones'])\n",
        "df2"
      ],
      "metadata": {
        "id": "gCNJ6YEg2Y6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tweet(id):\n",
        "  url=\"https://api.twitter.com/2/tweets?ids=\" + str(id)\n",
        "  response = requests.request(\"GET\", url, auth=bearer_oauth, params={'tweet.fields': 'author_id,entities'})\n",
        "  info = json.loads(response.text)\n",
        "  # print(info)\n",
        "  data.append(info['data'])"
      ],
      "metadata": {
        "id": "sC3v6GIQ2bD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(df2.usuario)):\n",
        "  if df2.tipo[i] == 'retweeted':\n",
        "    data = []\n",
        "    get_tweet(df2.id_tw_original[i])\n",
        "    df2.id[i] = df2.id_tw_original[i]\n",
        "    df2.usuario[i] = data[0][0]['author_id']\n",
        "    df2.tipo[i] = 'original'\n",
        "    df2.id_tw_original[i] = '-'\n",
        "    try:\n",
        "      men = []\n",
        "      for m in range(len(data[0][0]['entities']['mentions'])):\n",
        "        men.append(m['id'])\n",
        "      df2.menciones[i] = men\n",
        "    except:\n",
        "      df2.menciones[i] = '-'\n",
        "\n",
        "df2"
      ],
      "metadata": {
        "id": "aazRic0G2cDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_user_verified(id):\n",
        "  url=\"https://api.twitter.com/2/users?ids=\" + str(id)\n",
        "  response = requests.request(\"GET\", url, auth=bearer_oauth, params={'user.fields': 'verified'})\n",
        "  info = json.loads(response.text)\n",
        "  #print(info)\n",
        "  data.append(info['data'])"
      ],
      "metadata": {
        "id": "Qt39bhqN2e6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "verificados = []\n",
        "for i in range(len(df2.usuario)):\n",
        "  if df2.menciones[i] != '-':\n",
        "    data = []\n",
        "    get_user_verified(df2.usuario[i])\n",
        "    verificados.append(data[0][0]['verified'])\n",
        "  else:\n",
        "    verificados.append('-')\n",
        "df2['user_verified'] = verificados\n",
        "df2"
      ],
      "metadata": {
        "id": "h-kkTwiH2f5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = [[], [], [], [], [], [], [], [], [], [], []]\n",
        "for i in range(len(df2['usuario'])):\n",
        "  if df2['user_verified'][i] != True:\n",
        "    t[0].append(df2['id'][i])\n",
        "    t[1].append(df2['usuario'][i])\n",
        "    t[2].append(df2['tipo'][i])\n",
        "    t[3].append(df2['id_tw_original'][i])\n",
        "    t[4].append(df2['label'][i])\n",
        "    t[5].append(df2['menciones'][i])\n",
        "    t[6].append(df2['user_verified'][i])\n",
        "\n",
        "d = {'id':t[0], 'usuario':t[1], 'tipo':t[2], 'id_tw_original':t[3], 'label':t[4], 'menciones':t[5], 'user_verified':t[6]}\n",
        "df3 = pd.DataFrame(d, columns=['id', 'usuario', 'tipo', 'id_tw_original', 'label', 'menciones', 'user_verified'])\n",
        "df3"
      ],
      "metadata": {
        "id": "xgEFjydn2h7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fase II"
      ],
      "metadata": {
        "id": "wfbyipX62i8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# obtención datos del usuario\n",
        "def get_user(id):\n",
        "  url=\"https://api.twitter.com/2/users?ids=\" + str(996403244)\n",
        "  response = requests.request(\"GET\", url, auth=bearer_oauth, params={'user.fields': 'public_metrics,location,profile_image_url,username,verified'})\n",
        "  info = json.loads(response.text)\n",
        "  print(info)\n",
        "  data.append(info['data'])"
      ],
      "metadata": {
        "id": "bEo5cOsD2kHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "get_user(996403244)\n",
        "print(data)"
      ],
      "metadata": {
        "id": "3_V6M4aj2lZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "table = [[], [], [], [], [], []]\n",
        "table[0].append(data[0][0]['id'])\n",
        "table[1].append(data[0][0]['name'])\n",
        "table[2].append(data[0][0]['username'])\n",
        "table[3].append(data[0][0]['verified'])\n",
        "if data[0][0]['profile_image_url'] == 'https://abs.twimg.com/sticky/default_profile_images/default_profile_normal.png':\n",
        "  table[4].append('-')\n",
        "else:\n",
        "  table[4].append(data[0][0]['id'])\n",
        "table[5].append(data[0][0]['public_metrics']['followers_count'])\n",
        "d = {'id':table[0], 'name':table[1], 'username':table[2], 'verified':table[3], 'profile_img':table[4], 'followers':table[5]}\n",
        "user = pd.DataFrame(d, columns=['id', 'name', 'username', 'verified', 'profile_img', 'followers'])\n",
        "user"
      ],
      "metadata": {
        "id": "xjQ40DMS2mlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# obtención 100 tweets del usuario\n",
        "def get_user_tweets():\n",
        "  url=\"https://api.twitter.com/2/users/996403244/tweets?\"\n",
        "  response = requests.request(\"GET\", url, auth=bearer_oauth, params={'max_results': 100,\n",
        "                                                                      'user.fields': 'location,profile_image_url,username,verified',\n",
        "                                                                     'tweet.fields': 'public_metrics,created_at,entities,referenced_tweets,text'})\n",
        "  info2 = json.loads(response.text)\n",
        "  #print(info)\n",
        "  data2.append(info2['data'])"
      ],
      "metadata": {
        "id": "yLEZpso82oV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data2=[]\n",
        "get_user_tweets()\n",
        "len(data2[0])\n",
        "data2[0]"
      ],
      "metadata": {
        "id": "CmdiDXDw2p8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "table2 = [[]]\n",
        "for i in range(len(data2[0])):\n",
        "  table2[0].append(data2[0][i]['text'])\n",
        "t = {'tweet':table2[0]}\n",
        "tweets = pd.DataFrame(t, columns=['tweet', 'label'])\n",
        "tweets.head()"
      ],
      "metadata": {
        "id": "5kRASt5X2rEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets['cleaned'] = tweets['tweet'].apply(lambda x: contractions.fix(x, slang=True))\n",
        "tweets['cleaned'] = tweets['cleaned'].apply(lambda x: demoji.replace_with_desc(x))\n",
        "tweets['cleaned'] = tweets['cleaned'].str.replace(\"RT @[\\w]*\", \" \", regex=True)\n",
        "tweets['cleaned'] = tweets['cleaned'].str.replace(\"@[\\w]*\", \" \", regex=True)\n",
        "tweets['cleaned'] = tweets['cleaned'].str.replace(\"\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*\", \" \", regex=True)\n",
        "tweets['cleaned'] = tweets['cleaned'].str.replace(\"[^a-zA-Z]\", \" \", regex=True)"
      ],
      "metadata": {
        "id": "XzLdS24-2s0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_data = label_sentences(tweets.cleaned, 'Test')"
      ],
      "metadata": {
        "id": "245WnN252uMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, min_count=1, alpha=0.065, min_alpha=0.065)\n",
        "model_dbow.build_vocab([x for x in tqdm(all_data)])"
      ],
      "metadata": {
        "id": "kE7cari12vmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectors_dbow = get_vectors(model_dbow, len(tweets['tweet']), 300, 'Test')"
      ],
      "metadata": {
        "id": "2HmS2Tp82wrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(100):\n",
        "  if classifier.predict(vectors_dbow)[i].startswith(\"POS\"):\n",
        "    tweets['label'][i] = 0\n",
        "  else:\n",
        "    tweets['label'][i] = 1"
      ],
      "metadata": {
        "id": "b8mmFFXR2x3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bad_tweets = 0\n",
        "for i in range(100):\n",
        "  if tweets['label'][i] == 1:\n",
        "    bad_tweets = bad_tweets + 1\n",
        "\n",
        "bad_tweets"
      ],
      "metadata": {
        "id": "IzvIlsiI2zNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user['bad tweets %'] = bad_tweets\n",
        "user"
      ],
      "metadata": {
        "id": "gs3X9jhZ20Zo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data2"
      ],
      "metadata": {
        "id": "xpwhSEWs2110"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "table_h = [[]]\n",
        "for i in range(len(data2[0])):\n",
        "  try:\n",
        "      for h in data2[0][i]['entities']['hashtags']:\n",
        "        table_h[0].append(h['tag'])\n",
        "  except:\n",
        "    continue\n",
        "h = {'mentions':table_h[0]}\n",
        "hashtags = pd.DataFrame(h, columns=['hashtags', 'number'])\n",
        "hashtags.head()"
      ],
      "metadata": {
        "id": "_eFjxvT-221g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "table_m = [[]]\n",
        "for i in range(len(data2[0])):\n",
        "  try:\n",
        "     # mentions = []\n",
        "      for m in data2[0][i]['entities']['mentions']:\n",
        "        table_m[0].append(m['id'])\n",
        "      #  mentions.append(m['id'])\n",
        "      #table_m[0].append(mentions)\n",
        "  except:\n",
        "    continue\n",
        "m = {'mentions':table_m[0]}\n",
        "mentions = pd.DataFrame(m, columns=['mentions', 'number'])\n",
        "mentions['number'] = 1\n"
      ],
      "metadata": {
        "id": "JJRSv78Q24xd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(data2[0])):\n",
        "  for j in range(len(data2[0])):\n",
        "    if (i!=j):\n",
        "      if (mentions['mentions'][i]==mentions['mentions'][j]):\n",
        "        if(j>i):\n",
        "          mentions['number'][i] = mentions['number'][i] +1\n",
        "          mentions['number'][j]=0"
      ],
      "metadata": {
        "id": "K9HFK8cR256o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mentions.sort_values('number', ascending=False)"
      ],
      "metadata": {
        "id": "6fUZX9No26gG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_user_verified(id):\n",
        "  url=\"https://api.twitter.com/2/users?ids=\" + str(id)\n",
        "  response = requests.request(\"GET\", url, auth=bearer_oauth, params={'user.fields': 'verified'})\n",
        "  info3 = json.loads(response.text)\n",
        "  print(info3)\n",
        "  data3.append(info3['data'])"
      ],
      "metadata": {
        "id": "fatVvbMl28y8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "verificados = []\n",
        "for i in range(len(mentions.mentions)):\n",
        "  if mentions.mentions[i] != '-':\n",
        "    data3 = []\n",
        "    get_user_verified(mentions.mentions[i])\n",
        "    verificados.append(data3[0][0]['verified'])\n",
        "  else:\n",
        "    verificados.append('-')\n",
        "mentions['verified'] = verificados\n",
        "mentions"
      ],
      "metadata": {
        "id": "CmiSAANz29-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "verif = 0\n",
        "for i in range(195):\n",
        "  if mentions['verified'][i]==True:\n",
        "    verif=verif+1\n",
        "\n",
        "user['verified_%'] = verif*100/195\n",
        "user"
      ],
      "metadata": {
        "id": "KWcWLbE52_QU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets"
      ],
      "metadata": {
        "id": "ShPjir9s3AR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data2[0][1]['public_metrics']"
      ],
      "metadata": {
        "id": "n48j7ljT3Ale"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "table2 = [[], [], [], [], [], []]\n",
        "for i in range(len(data2[0])):\n",
        "  table2[0].append(data2[0][i]['referenced_tweets'][0]['type'])\n",
        "  table2[1].append(data2[0][i]['public_metrics']['like_count'])\n",
        "  table2[2].append(data2[0][i]['public_metrics']['quote_count'])\n",
        "  table2[3].append(data2[0][i]['public_metrics']['reply_count'])\n",
        "  table2[4].append(data2[0][i]['public_metrics']['retweet_count'])\n",
        "  table2[5].append(data2[0][i]['created_at'])\n",
        "t = {'type':table2[0], 'likes':table2[1], 'quotes':table2[2], 'replies':table2[3], 'retweets':table2[4], 'created_at':table2[5]}\n",
        "tweets = pd.DataFrame(t, columns=['type', 'likes', 'quotes', 'replies', 'retweets', 'created_at'])\n",
        "tweets"
      ],
      "metadata": {
        "id": "VsHxsNlJ3CBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "replies = 0\n",
        "quotes = 0\n",
        "original = 0\n",
        "retweet = 0\n",
        "for i in range(100):\n",
        "  if tweets['type'][i] == 'replied_to':\n",
        "    replies = replies +1\n",
        "    continue\n",
        "  if tweets['type'][i] == 'quoted':\n",
        "    quotes = quotes +1\n",
        "    continue\n",
        "  if tweets['type'][i] == 'retweeted':\n",
        "    retweet = retweet +1\n",
        "    continue\n",
        "  else:\n",
        "    original = original +1\n",
        "\n",
        "print('De los 100 tweets, ',original,' son tweets originales, ', quotes, ' son citas a otras publicaciones, ', retweet, ' son retweets y ', replies, ' son respuestas a otros usuarios.')"
      ],
      "metadata": {
        "id": "nrz4M14q3DJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "likes = tweets.likes.sum()\n",
        "quotes = tweets.quotes.sum()\n",
        "replies = tweets.replies.sum()\n",
        "retweets = tweets.retweets.sum()\n",
        "print('Sus 100 tweets han obtenido en total', likes, ' likes, ', quotes, ' quotes, ', replies, ' replies y ',retweets, ' retweets.' )"
      ],
      "metadata": {
        "id": "LK1JJL-I3Esu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = 0\n",
        "for i in range(100):\n",
        "  if tweets['created_at'][i].startswith('2022-06-20'):\n",
        "    t = t +1\n",
        "\n",
        "t"
      ],
      "metadata": {
        "id": "yTtSRcuH3F5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user['tipo tweets'] = 'respuestas'\n",
        "user['impacto'] = 'despreciable'\n",
        "user['tweets/día'] = 'mínimo 100'"
      ],
      "metadata": {
        "id": "9lkRJR2c3HCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user"
      ],
      "metadata": {
        "id": "Ne2M5wvD3IYt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}